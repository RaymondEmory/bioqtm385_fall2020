{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bioqtm385-inclass-02-pca.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb9Vyq8I5E8q",
        "colab_type": "text"
      },
      "source": [
        "##**BIO/QTM 385: In class exercise for Monday, August 31st** \n",
        "\n",
        "(answers will be the part of Assignment #2, Due 9/16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8XJZSi05PY-",
        "colab_type": "text"
      },
      "source": [
        "<font color='green'>**Enter your names and group number here.**  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYx7ktKVq86i",
        "colab_type": "text"
      },
      "source": [
        "*This notebook contains modified excerpts from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook). The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT).  In addition, aspects have been adapted from the Neuromatch course materials [here](https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/README.md).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUNFMJre5b1G",
        "colab_type": "text"
      },
      "source": [
        "For this exercise, we will explore Principal Components Analysis (PCA) on both simulated and real data.  Like last time, all questions to be answered will be in <font color=\"blue\"> blue</font> and places to write your answers will be in <font color=\"green\"> green</font>.  Sean and I will be moving through the breakout rooms to help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dePgNvVqcCDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import various useful packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.random as random\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqaPipvXcb5D",
        "colab_type": "text"
      },
      "source": [
        "## Introducing Principal Components Analysis\n",
        "\n",
        "As we saw in lecture, Principal Components Analysis (PCA) is a fast and broadly-applicable unsupervised method for dimensionality reduction, finding a rotation of a data set into a low-dimensional space that captures as much linear correlation as possible.  While this is somewhat of a mouthful, its behavior is easiest to visualize by looking at a two-dimensional dataset.\n",
        "Consider the following 200 points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEorN2xFf5_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng = random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.plot(X[:, 0], X[:, 1],'.')\n",
        "plt.axis('equal');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWt31JdDgQ5x",
        "colab_type": "text"
      },
      "source": [
        "By eye, it is clear that there is a nearly linear relationship between the x and y variables.  This is reminiscent of linear regression, but the problem here is slightly different: rather than attempting to *predict* the y values from the x values, the unsupervised learning problem attempts to learn about the *relationship* between the x and y values.\n",
        "\n",
        "In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n",
        "Using Scikit-Learn's ``PCA`` estimator, we can compute this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-apgvkBf-uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn3xUnO2iCDx",
        "colab_type": "text"
      },
      "source": [
        "The PCA fit learns some useful quantities from the data, most importantly the \"components\" and \"explained variance\" (i.e., $\\frac{\\lambda_k}{\\sum_{i=1}^d \\lambda_i}$, where $\\lambda_k$ is the $k^{th}$ largest eigenvalue):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0kco3fah4l-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pca.components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjsa3ZR9iOUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pca.explained_variance_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdDNl5wiR23",
        "colab_type": "text"
      },
      "source": [
        "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSadSRMBiPfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->',\n",
        "                    linewidth=2,\n",
        "                    shrinkA=0, shrinkB=0,color=\"black\")\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.plot(X[:, 0], X[:, 1],'.',alpha=0.8)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFgW4vsEjzWV",
        "colab_type": "text"
      },
      "source": [
        "These vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the dataâ€”more precisely, it is proportional to the eigenvalue that corresponds to the plotted eigenvector.  The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
        "\n",
        "If we plot these principal components beside the original data, we see the plots shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNfBITi8izXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "# plot data\n",
        "ax[0].plot(X[:, 0], X[:, 1],'.', alpha=0.8)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
        "ax[0].axis('equal');\n",
        "ax[0].set(xlabel='x', ylabel='y', title='input')\n",
        "\n",
        "# plot principal components\n",
        "X_pca = pca.transform(X)\n",
        "ax[1].plot(X_pca[:, 0], X_pca[:, 1],'.', alpha=0.8)\n",
        "draw_vector([0, 0], [0, -3*np.sqrt(pca.explained_variance_[1])], ax=ax[1])\n",
        "draw_vector([0, 0], [3, 0], ax=ax[1])\n",
        "ax[1].axis('equal')\n",
        "ax[1].set(xlabel='component 1', ylabel='component 2',\n",
        "          title='principal components',\n",
        "          xlim=(-5, 5), ylim=(-3, 3.1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRcUILJukwLS",
        "colab_type": "text"
      },
      "source": [
        "You can see that now, almost all of the data lie near the line.  Thus, we can see that using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
        "\n",
        "Here is an example of using PCA as a dimensionality reduction transform - performing principle components, taking only the first eigenvalue projection, and then histogramming the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAms4AObkTMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "Y = pca.transform(X)\n",
        "plt.hist(Y[:,0],20,density=True)\n",
        "plt.xlabel('Projection onto the direction with the largest eigenvalue')\n",
        "plt.ylabel('Probability Density')\n",
        "print(\"original shape:   \", X.shape)\n",
        "print(\"transformed shape:\", Y.shape)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCi3rpDXayGW",
        "colab_type": "text"
      },
      "source": [
        "##Syntax for PCA in ```sklearn.decomposition```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzVlwqrL97d",
        "colab_type": "text"
      },
      "source": [
        "As is often seen in python, the ```sklearn``` version of PCA is an example of a *class* data structure.  This type of data structure is rather convenient, as it allows the users to manipulate variables in a systematic manner.  Although a complete description and some examples are [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform), we will go over the important basics in this tutorial.  As we move towards using other dimensionality reduction techniques, you will find that ```sklearn``` often will use similar syntax for those methods as well.\n",
        "\n",
        "The first step will be to initialize a ```PCA``` class like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ckly0_Mo60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_example = PCA()\n",
        "#Note, there are a number of options, such as limiting the number of components \n",
        "#   or equalizing the variance that are described in the link in the previous\n",
        "#   paragraph.  We will not need them today, but they might be useful in the future"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM79nKX9NIBZ",
        "colab_type": "text"
      },
      "source": [
        "This class (here, ```pca_example```) will contain several helper functions and will store variables that result from applying PCA to that dataset.\n",
        "\n",
        "Now, let's say that we have a dataset, $X\\in\\Re^{N\\times d}$, to which we would like to apply PCA.  To achieve this, we would use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHGteH-9OCTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_example.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivgRUaqxOKQT",
        "colab_type": "text"
      },
      "source": [
        "Note here, that we didn't create a new variable.  ```pca_example``` will carry all of the results along with it.  \n",
        "\n",
        "For example, to return the eigenvalues (sorted from largest to smallest) from running PCA, we can type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0iBujqO4_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_pca_eigenvalues = pca_example.explained_variance_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umbbBEObPIGR",
        "colab_type": "text"
      },
      "source": [
        "This will return a $d$-dimensional numpy array of eigenvalues.\n",
        "\n",
        "Similarly, if you would like the relative variance explained in each mode $\\left(\\frac{\\lambda_k}{\\sum_{\\ell=1}^d \\lambda_\\ell} \\right)$, then you can enter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAno6mP8PHYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_pca_relative_variance = pca_example.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXFsFVKXPzXi",
        "colab_type": "text"
      },
      "source": [
        "In addition, ```pca_example.components_```, returns the eigenvectors (as row vectors) resulting from applying PCA, ordered in the same manner as the eigenvalues.  For example, the first eigenvector would be ```pca_example.components_[0,:]```.\n",
        "\n",
        "Lastly, we often would like to calculate the projections onto the first $m$ eigenvectors ($\\{\\vec{x_i}\\cdot \\hat{v}_k\\}_{k=1,\\ldots , m}$).  These projections are the low-dimensional representation that we were aiming to find in the first place.  To return these values, we can use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4YueyZcRBUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "projections = example_pca.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA9R2aWvRL2F",
        "colab_type": "text"
      },
      "source": [
        "where ```X``` is the original dataset that you started out with (or another data set altogether if you wanted) and ```projections``` is an $N\\times d$ matrix of the projections onto the eigenvectors.\n",
        "\n",
        "If you want to test these functions out, I would recommend using ```X = random.randn(N,d)``` or something similar, and you can see how they work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyVPxgoKqNzj",
        "colab_type": "text"
      },
      "source": [
        "##PCA on hand-written digits\n",
        "\n",
        "Now to try PCA out on some real data.  The MNIST dataset consists of a 70,000 images of individual handwritten digits. Each image is a 28x28 pixel grayscale image. For convenience, each 28x28 pixel image is often unravelled into a single 784 (=28*28) element vector, so that the whole dataset is represented as a 70,000 x 784 matrix. Each row represents a different image, and each column represents a different pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-ODXiMq9N9",
        "colab_type": "text"
      },
      "source": [
        "First, press enter on the cell below, as it will introduce several MNIST-specific helper functions into the workspace.  If you are curious, you can open it up and see what the code is doing after the tutorial today!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT8sOg-Jqu0q",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "\n",
        "\n",
        "def plot_MNIST_reconstruction(X, X_reconstructed):\n",
        "  \"\"\"\n",
        "  Plots 9 images in the MNIST dataset side-by-side with the reconstructed\n",
        "  images.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats)               : Data matrix each column\n",
        "                                              corresponds to a different\n",
        "                                              random variable\n",
        "    X_reconstructed (numpy array of floats) : Data matrix each column\n",
        "                                              corresponds to a different\n",
        "                                              random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  ax = plt.subplot(121)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
        "                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n",
        "                 vmin=0, vmax=255)\n",
        "  plt.xlim((3 * 28, 0))\n",
        "  plt.ylim((3 * 28, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.title('Data')\n",
        "  plt.clim([0, 250])\n",
        "  ax = plt.subplot(122)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(np.real(X_reconstructed[k, :]), (28, 28)),\n",
        "                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n",
        "                 vmin=0, vmax=255)\n",
        "  plt.xlim((3 * 28, 0))\n",
        "  plt.ylim((3 * 28, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.clim([0, 250])\n",
        "  plt.title('Reconstructed')\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "def plot_MNIST_sample(X):\n",
        "  \"\"\"\n",
        "  Plots 9 images in the MNIST dataset.\n",
        "\n",
        "  Args:\n",
        "     X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                 different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  k = 0\n",
        "  for k1 in range(10):\n",
        "    for k2 in range(10):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
        "                 extent=[(k1 + 1) * 28, k1 * 28, (k2+1) * 28, k2 * 28],\n",
        "                 vmin=0, vmax=255)\n",
        "  plt.xlim((10 * 28, 0))\n",
        "  plt.ylim((10 * 28, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  plt.clim([0, 255])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_MNIST_weights(weights):\n",
        "  \"\"\"\n",
        "  Visualize PCA basis vector weights for MNIST. Red = positive weights,\n",
        "  blue = negative weights, white = zero weight.\n",
        "\n",
        "  Args:\n",
        "     weights (numpy array of floats) : PCA basis vector\n",
        "\n",
        "  Returns:\n",
        "     Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  cmap = plt.cm.get_cmap('seismic')\n",
        "  Z = np.zeros((28*4,28*6))\n",
        "  k = 0\n",
        "  for i in range(4):\n",
        "    xstart = 0 + i*28\n",
        "    xend = (i+1)*28\n",
        "    for j in range(6):\n",
        "      ystart = 0 + j*28\n",
        "      yend = (j+1)*28\n",
        "      Z[xstart:xend,ystart:yend] = np.real(np.reshape(weights[k,:], (28, 28)))\n",
        "      k += 1\n",
        "  #plt.imshow(np.real(np.reshape(weights[i,:], (28, 28))), cmap=cmap)\n",
        "  plt.imshow(Z,cmap=cmap)\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  plt.clim(-.15, .15)\n",
        "  plt.colorbar(ticks=[-.15, -.1, -.05, 0, .05, .1, .15])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def add_noise(X, frac_noisy_pixels):\n",
        "  \"\"\"\n",
        "  Randomly corrupts a fraction of the pixels by setting them to random values.\n",
        "\n",
        "  Args:\n",
        "     X (numpy array of floats)  : Data matrix\n",
        "     frac_noisy_pixels (scalar) : Fraction of noisy pixels\n",
        "\n",
        "  Returns:\n",
        "     (numpy array of floats)    : Data matrix + noise\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  X_noisy = np.reshape(X, (X.shape[0] * X.shape[1]))\n",
        "  N_noise_ixs = int(X_noisy.shape[0] * frac_noisy_pixels)\n",
        "  noise_ixs = np.random.choice(X_noisy.shape[0], size=N_noise_ixs,\n",
        "                               replace=False)\n",
        "  X_noisy[noise_ixs] = np.random.uniform(0, 255, noise_ixs.shape)\n",
        "  X_noisy = np.reshape(X_noisy, (X.shape[0], X.shape[1]))\n",
        "\n",
        "  return X_noisy\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_gy0Cx6q6Yt",
        "colab_type": "text"
      },
      "source": [
        "Now enter the following cell to load the MNIST dataset.  To provide a sense of the data, the code will also plot 100 of the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsaZopHRqkuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml(name='mnist_784')\n",
        "digitData = mnist.data\n",
        "plot_MNIST_sample(digitData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-cLW20mrTx5",
        "colab_type": "text"
      },
      "source": [
        "### MNIST eigenvalues\n",
        "\n",
        "The MNIST dataset has an measured dimensionality of 784! To make sense of this data, we'll need to use dimensionality reduction. But first, we need to determine the intrinsic dimensionality, $m$, of the data. One way to do this is to look for an \"elbow\" in the eigenvalue plot, to determine which eigenvalues are signficant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRDdLhFerrd2",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\"> Question #1: Apply PCA to this data set (call the class ```digitPCA```), and plot the 100 largest eigenvalues.  Can you identify an \"elbow\" in the eigenvalue plot?  If so, where do you think it is located? </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XMqUAjyuAZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Type code for Question #1 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVmGZS8TuQkQ",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\">Can you identify an \"elbow\" in the eigenvalue plot?  If so, where do you think it is located?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbEsSbdPuhg1",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\"> Question #2: Now plot the cumulative fraction of variance explained as a function of the number of dimensions included from 1 up to 100 dimensions (```np.cumsum()``` will be helpful).  How many dimensions do you need to explain 50% of the variance? 80%? 90%? Compare these numbers to your answer for Question #1.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsvFeYhEvZIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Type code for Question #2 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-U6Cprevgoj",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\"> How many dimensions do you need to explain 50% of the variance? 80%? 90%?  Compare these numbers to your answer for Question #1.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jvz6tg9Dw6M",
        "colab_type": "text"
      },
      "source": [
        "###Shuffled eigenvalues\n",
        "\n",
        "One standard way to determine how many modes one should use in a PCA decomposition is to compare the found eigenvalues with the eigenvalues from applying PCA to the same data set, but with each column shuffled independently from one another. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJoGTSzxHAWs",
        "colab_type": "text"
      },
      "source": [
        "We start by making a copy of our original array (to make sure that we don't overwrite it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cIdnJnZGbcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffledDigitData = np.copy(digitData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HS7oMMPHQoC",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">Question #3: Shuffle this copied data matrix so that each column is shuffled independently (```random.shuffle()``` will be useful here - see the previous in-class exercise for an example)  and apply PCA to this new, shuffled, data set.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skWawzadKPDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your code for Question #3 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kQilFwLKXb5",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">Question #4: Plot the first 100 eigenvalues from both the actual data PCA and the shuffled data PCA on the same plot (but with different colors).  Based on this plot, how many modes do you think we should keep?  Why?</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMEyOXPXKvJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your code for Question #4 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq2lm3IHK10j",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\"> Based on this plot, how many modes do you think we should keep? Why? </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEwGBQusLEzI",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">Question #5: Is this number of modes larger or smaller than you would have expected before performing the analysis?  Why?  What assumptions does PCA make that could be affecting this result?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfpF1zNRLmh7",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\"> Enter your answer to Question #5 here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl8ZZi-_ayVH",
        "colab_type": "text"
      },
      "source": [
        "###MNIST eigenvectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJJHYhwPbBL7",
        "colab_type": "text"
      },
      "source": [
        "As well as the eigenvalues, we can also look at the eigevectors as well.  However, because the eigenvectors are in a high dimension ($d=784$!), they are hard to visualize.  Fortunately, however, we can look at these vectors not as vectors, per se, but as images.\n",
        "\n",
        "The code below will plot the first 24 eigenvectors from left to right, then top to bottom (i.e, [[1,2,3,4,5,6],[7,8,9,10,11,12],...,...), with red and blue depicting opposite directions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi4aGrFlbtO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "plot_MNIST_weights(digitPCA.components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O_SzB1SbzoC",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">Question # 6: Describe how the eigenvectors are changing as we go from a low mode (top left) to higher modes (top right).  Do they look like numbers? Why or why not might this be?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhRp1CaBcQ3U",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\">Write your answer to Question # 6 here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Klz6XvmDmej",
        "colab_type": "text"
      },
      "source": [
        "###MNIST projections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQlHCaJVa6IZ",
        "colab_type": "text"
      },
      "source": [
        "For the last section, we will look at the projections onto the PCA eigenvectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3yWJjwx9fj4",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"blue\">Question 7: Use the ```transform()``` member function of ```PCA``` to return the projections of the high-dimensional points onto a low-dimensional space.  Call these projections, ```projections```.Plot a histogram of the projections onto the first eigenvector.  Does the histogram look symmetric?  What might this symmetry or asymmetry tell us about the data set?</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki1_CHSvtoxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Type code for Question #7 here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDPEJjun9dff",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\">Does the histogram look symmetric? What might this symmetry or asymmetry tell us about the data set?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0E68TZ3_Hr4",
        "colab_type": "text"
      },
      "source": [
        "Given these projections, we can now see if the low-dimensional representation that PCA provides can tell us something about the structure of the data.  The code below picks a random number of points from the data set (otherwise it would be way to hard to see!) and plots the projections on the two different eigenvectors against each other.  The points are colored based on the known digit values (as per the colorbar).  You can change the number of random points, the mode on the x-axis, and the mode on the y-axis.  The default is to plot the first to projections against each other (```0``` and ```1```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3_DDBKh4NFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numToPlot = 2000 #number of random samples to choose (I recommend 2000)\n",
        "mode1 = 0 # PCA direction to plot on the x-axis of the plot below\n",
        "mode2 = 1 # PCA direction to plot on the y-axis of the plot below\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "idx = random.choice(range(len(projections[:,0])),numToPlot)\n",
        "plt.scatter(projections[idx,mode1],projections[idx,mode2],c=mnist.target.astype(np.int)[idx],edgecolor=\"none\",cmap=plt.cm.get_cmap('nipy_spectral'))\n",
        "plt.colorbar();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej3VvTWvBIEi",
        "colab_type": "text"
      },
      "source": [
        "<font color = \"blue\">Question 8: Run the code above for ```mode1 = 0``` and ```mode2 = 1```.  Is it possible to easily tell apart numbers in this representation?  Which number(s) are easiest to tell apart and which number(s) are hardest?  Given how the numbers look, do these observations make sense to you?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLeYcrseBxYV",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\">Type answer for Question #8 here.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1uH0MhOB8w1",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">Question 9: Now run this code a few times (you can copy and paste below) three different combinations of ```mode1``` and ```mode2``` (keeping ```mode1``` < ```mode2``` and ```mode2``` $\\le 10$).  Are the numbers more or less distinguishable now?  Why might that be?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jk8Oq3MDN8-",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"green\">Type answer for Question #9 here.</font>"
      ]
    }
  ]
}