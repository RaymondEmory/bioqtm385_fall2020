{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bioqtm-inclass-07-sloppymodels.ipynb","provenance":[{"file_id":"16HrWKIWadhfkAwuUIfK_kFIVIIDIcw-L","timestamp":1635260196342},{"file_id":"1BdX02THlM8LZfNLVideWwSwgxrqRkPT1","timestamp":1602687026192}],"authorship_tag":"ABX9TyOcC1qpSJxJAwB4juQYj11U"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"chIzq3l5zzql"},"source":["##**BIO/QTM 385: In class exercise for Wednesday, October 27th** \n","\n","\n","(answers will be the part of Assignment #4, Due 11/3)"]},{"cell_type":"markdown","metadata":{"id":"bk_xqsDGz8n0"},"source":["As always, all questions to be answered will be in <font color=\"blue\"> blue</font> and places to write your answers will be in <font color=\"green\"> green</font>."]},{"cell_type":"code","metadata":{"id":"8EMGUbCEztI8"},"source":["#import useful libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy.random as random\n","import numpy.linalg as linalg\n","import scipy.optimize as optimize\n","from scipy.optimize import least_squares"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlLXNuPD0Reu","cellView":"form"},"source":["#@title Figure Settings\n","import ipywidgets as widgets       # interactive display\n","%config InlineBackend.figure_format = 'retina'\n","plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dwPJ9E4q0ZLp"},"source":["##The Fisher Information Matrix and Sloppy Models"]},{"cell_type":"markdown","metadata":{"id":"CTikO9rg0fG0"},"source":["As we've seen several times by now, when we're trying to infer the parameters ($\\theta\\in \\mathcal{R}^p$) of a model, $\\mathcal{M}$, based on data, $X$, we can calculate the *posterior distribution* of our parameter values given the the data via:\n","\\begin{equation}\n","p(\\theta \\vert X) \\propto p(X \\vert \\theta) p(\\theta),\n","\\end{equation}\n","where $p(X \\vert \\theta)$ is the *likelihood* of observing the data given a particular set of parameter values and $p(\\theta)$ is our *prior* distribution over parameter space (we'll ignore the normalization from $p(X)$ for now).\n","\n","As we saw in class, though, often the likelihood function often is not able to precisely specify one or many of the model's parameters.  We can quantify this notion using the Fisher Information Matrix (FIM) - which we will refer to as $g$.  In this matrix, each entry is the average curvature of the log-likelihood function in their respective parameter directions.  If we have a data set with $N$ data points, and $\\theta_i$ and $\\theta_j$ are two of the model parameters, then the FIM is given by:\n","\\begin{equation}\n","g_{i,j} = -\\sum_{k=1}^N p(x_k\\vert\\theta) \\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j}\\log p(x_k\\vert\\theta).\n","\\end{equation}\n","\n","Note how this matrix is symmetric ($\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}$) and real-valued, so it's a positive-definite matrix with (all eigenvales are greater than zero).  Thus, we can look at the eigenvalue structure.  \n","\n","Here, large eigenvalues correspond to eigenvectors in parameter space that have a large curvature in the log-likelihood function - implying that we can perform inference well.  Small eigenvalues correspond to eigenvectors in parameter space that have small curvatures - implying that it will be very difficult to infer parameters along those directions.\n","\n","In the case of least-squares fitting to a model function $f(t;\\theta)$, we can define the FIM using:\n","\\begin{equation}\n","g_{i,j} = \\sum_{k=1}^N \\frac{\\partial f(t_k;\\theta)}{\\partial \\theta_i}\\frac{\\partial f(t_k;\\theta)}{\\partial \\theta_j} = (J^T(\\theta) J(\\theta))_{i,j},\n","\\end{equation}\n","where $J(\\theta)$ is the Jacobian matrix ($J_{k,i} = \\frac{\\partial f(t_k;\\theta)}{\\partial \\theta_i}$)."]},{"cell_type":"markdown","metadata":{"id":"6KZBBYvK-8YC"},"source":["##An Example: Mixtures of Exponentials"]},{"cell_type":"markdown","metadata":{"id":"kvgZ_Ksa_CJA"},"source":["For this part of the exercise, you will try to infer the parameters from a model of the form:\n","\\begin{equation}\n","y(t) = \\sum_{i=1}^d A_i e^{-t/\\tau_i}.\n","\\end{equation}\n","The code below defines a function, ```multiExponential(t,As,taus)```, that generates outputs, ```y``` for input array ```t``` (and parameter arrays ```As``` and ```taus```), and ```createRandomExponentialModel(d)``` that will create a random model of order ```d``` (as defined in the equation above)."]},{"cell_type":"code","metadata":{"id":"56hiq0r90eeR"},"source":["def multiExponential(t,As,taus):\n","    y = np.zeros(len(t))\n","    L = len(As)\n","    for i in range(L):\n","        y += As[i]*np.exp(-t/taus[i])\n","    return y\n","\n","def createRandomExponentialModel(d,Amax=10,tauMax = 10):\n","  As = random.uniform(0,Amax,d)\n","  taus = random.uniform(0,tauMax,d)\n","  return As,taus"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3hagWXbBCwf"},"source":["<font color=blue>Question #1: Create a random exponential model with $d=7$ and plot its evaluation ($y$ vs. $t$) over ```np.arange(0,21,1)```.  Looking at this curve, would you expect to be able to infer all 14 parameters?  Why or why not?</font>"]},{"cell_type":"code","metadata":{"id":"HTiSvG6mA7h_"},"source":["#Type code for question #1 here.  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1utRrouFoxg"},"source":["<font color=\"green\"> Looking at this curve, would you expect to be able to infer all 14 parameters?  Why or why not? </font>"]},{"cell_type":"markdown","metadata":{"id":"YDKi71tSc6lQ"},"source":["The code below introduces a function, ```returnExponentialCurveFit(t,y,d)```, that fits a given data set (```t``` and ```y```) to the multi-exponential curve of order ```d``` above by minimizing the least squares function.  The output, ```yFit``` is the best-fit curve resulting from the fit."]},{"cell_type":"code","metadata":{"id":"5S-leOEKI4iB","executionInfo":{"status":"ok","timestamp":1635265096235,"user_tz":240,"elapsed":2,"user":{"displayName":"Gordon Berman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXcXAHsZG8aYsOgCwtUauajmgGF20J2T7z5FaJaA=s64","userId":"05355355452857904347"}}},"source":["def leastSquaresFunction_Exponentials(theta,t,y):\n","  d = int(np.shape(theta)[0]/2)\n","  As = np.exp(theta[:d])\n","  taus = np.exp(theta[d:])\n","  return multiExponential(t,As,taus) - y\n","\n","def returnExponentialCurveFit(t,y,d):\n","\n","  A0 = np.log(random.dirichlet(np.ones(d))*y[0])\n","  tau0 = np.log(random.uniform(0,np.max(t),size=d))\n","  theta0 = np.zeros(2*d)\n","  theta0[:d] = A0\n","  theta0[d:] = tau0\n","\n","  lsq = least_squares(leastSquaresFunction_Exponentials, theta0, args=(t, y))\n","\n","  Afit = np.exp(lsq.x[:d])\n","  tauFit = np.exp(lsq.x[d:])\n","\n","  yFit = multiExponential(t,Afit,tauFit)\n","\n","  return yFit"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfDgHbXZdRP3"},"source":["<font color=blue> Question #2: Run ```returnExponentialCurveFit()``` for $d=1, 3, 5,$ and $7$ using your data from Question #1.  Plot the resulting best fit curves (comparing to the original data) for each fit.  Visually (using ```plt.semilogy()```), which value of $d$ would you choose to model the data if you didn't know that it was actually generated from the $d=7$ equation?</font>"]},{"cell_type":"code","metadata":{"id":"rvE_oHcSQINz"},"source":["#Type your code for Question #2 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_dA8rP9veVqc"},"source":["<font color=green> Visually, which value of $d$ would you choose to model the data if you didn't know that it was actually generated from the $d=7$ equation? </font>"]},{"cell_type":"markdown","metadata":{"id":"aEDWBv4aegmv"},"source":["As shown in class, for a least-squares fitting problem with uniform measurement error, $\\sigma$, we can write the log-likelihood function via: \n","\\begin{equation}\n","\\mathcal{L}(x\\vert \\theta) = -\\frac{1}{2}\\log\\left(2\\pi \\sigma^2\\right) - \\frac{1}{2\\sigma^2}\\chi^2(\\theta,y),\n","\\end{equation}\n","where\n","\\begin{equation}\n","\\chi^2(\\theta,y) = \\sum_{k=1}^N \\left[y_i - \\hat{y}(x_i,\\theta) \\right]^2\n","\\end{equation}\n","and $\\hat{y}(x_i,\\theta)$ is the best-fit curve."]},{"cell_type":"markdown","metadata":{"id":"nAGCgdpuf8hw"},"source":["<font color=blue> Question #3: Add gaussian noise with standard deviation $\\sigma = .1$ to your previous value of ```y``` to make a new variable, ```z```.  Rembering that the BIC = $-2\\mathcal{L}(x\\vert \\theta,\\mathcal{M}) + p\\log N$, use Bayesian model selection to select the most likely value for $d$ given the data.  Explain your reasoning. Do you get $d=7$?  If not, why might this be? (You can assume that $\\sigma$ in the log-likelihood equation above is equal to 0.1)</font>"]},{"cell_type":"code","metadata":{"id":"zjMY3FMbg2gU"},"source":["#Type code for Question #3 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTTum0uVg5Wl"},"source":["<font color = green> Explain your reasoning. Do you get $d=7$?  If not, why might this be?</font>"]},{"cell_type":"markdown","metadata":{"id":"Zdvff6ByCf2v"},"source":["### The Fisher Information Matrix\n","\n","For the multi-exponential system, our partial derivatives are given by:\n","\\begin{eqnarray}\n","\\frac{\\partial y(t;\\theta)}{\\partial A_i} &=& e^{-t/\\tau_i} \\\\\n","\\frac{\\partial y(t;\\theta)}{\\partial \\tau_i } &=& \\frac{A_i t}{\\tau_i^2}e^{-t/\\tau_i}.\n","\\end{eqnarray}"]},{"cell_type":"markdown","metadata":{"id":"F4LcOhNqCGd1"},"source":["<font color=blue>Question #4: Write a function ```returnFisherInformationMultiExponentials(t,y,As,taus)``` that returns the Fisher Information Matrix for the multi-exponential model near the \"true\" value of the model ($d=7$).  Note that the output should be a $2d\\times 2d$ matrix.  </font>"]},{"cell_type":"code","metadata":{"id":"lUuLzNClB73b"},"source":["#Type code for Question #4 here\n","def returnFisherInformationMultiExponentials(t,y,As,taus):"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jsd_kFdUh4sh"},"source":["def makeEigPlot(vals):\n","    np.sort(vals)[::-1]\n","    L = np.shape(vals)[0]\n","    for i in range(L):\n","        plt.semilogy([0,1],[vals[i],vals[i]],'k-')\n","    plt.xlim([-.1, 1.1])\n","    plt.ylabel('Eigenvalues')\n","    plt.tick_params(\n","    axis='x',          \n","    which='both',      \n","    bottom=False,      \n","    top=False,         \n","    labelbottom=False) \n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RacUZSd_hP0n"},"source":["<font color=blue> Question #5: Calculate the eigenvalues and eigenvectors for the FIM using ```t```, ```As``` and ```taus``` from Question #1 and ```z``` from question #2 as your value for $y$.  Use ```makeEigPlot(eigenvalues)``` (in the cell above) to plot the eigenvalue spectrum for your FIM.  Does this model appear to be sloppy?  Is it identifiable?"]},{"cell_type":"code","metadata":{"id":"CIz3NFH0igNS"},"source":["#Enter your code for Question #5 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LlBd2reUilhC"},"source":["<font color=\"green\"> Does this model appear to be sloppy?  Is it identifiable? </font>"]},{"cell_type":"markdown","metadata":{"id":"iLkJjIUUltdL"},"source":["<font color=blue> Question #6: Visualize the eigenvectors from Question #5 using the method of your choosing.  Would it be easy to eliminate the smallest eigenvectors from your model?  Explain your reasoning."]},{"cell_type":"code","metadata":{"id":"faBkGeqPipej"},"source":["#Enter your code for Question #6 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfP3kYWQmLtG"},"source":["<font color=green> Would it be easy to eliminate the smallest eigenvectors from your model?  Explain your reasoning. </font>"]}]}