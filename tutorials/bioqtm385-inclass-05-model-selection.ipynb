{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bioqtm385-inclass-05-model-selection.ipynb","provenance":[{"file_id":"1yYVbpH9qDaGnknQpt3E1ffCEvjzFxE8T","timestamp":1601257785727}],"authorship_tag":"ABX9TyOXOEOq1oQ2HGo6JRrWoycu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"70i4JXPnrnUr"},"source":["##**BIO/QTM 385: In class exercise for Wednesday, October 6th & Wednesday, October 13th** \n","\n","\n","(answers will be the part of Assignment #3, Due 10/18)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W-066efi338i"},"source":["*Aspects of this notebook have been adapted from the Neuromatch course materials [here](https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/README.md).*"]},{"cell_type":"markdown","metadata":{"id":"f7khWkZbr59J"},"source":["As always, all questions to be answered will be in <font color=\"blue\"> blue</font> and places to write your answers will be in <font color=\"green\"> green</font>."]},{"cell_type":"code","metadata":{"id":"4NTCHcYNrmhZ"},"source":["#import useful libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy.random as random\n","from scipy.optimize import minimize\n","from scipy.optimize import fsolve\n","import pandas as pd\n","\n","#importing dimensionality reduction packages\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import NMF\n","from sklearn.manifold import MDS\n","from sklearn.manifold import Isomap \n","from sklearn.manifold import LocallyLinearEmbedding\n","from sklearn.manifold import TSNE\n","!pip install umap-learn[plot]\n","import umap\n","\n","#importing clustering packages\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8j-KZIeH9Qv","cellView":"form"},"source":["#@title Figure Settings\n","import ipywidgets as widgets       # interactive display\n","%config InlineBackend.figure_format = 'retina'\n","plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wc1w0EmmH4pB"},"source":["## Model selection\n","\n","Today, we will be looking at model selection -- how to select an individual model out of a set of many potential options.  The goal is to select a model that not only describes the data that we have, but also the the data that we would expect to get if we were to either collect more or to repeat the experiment.  This process forces us to confront many of the key trade-offs that we have seen so far in class.  These include representation vs. fidelity, incorporating new information vs. trusting prior knowledge, and a new trade-off: bias vs. variance.  We will start here. "]},{"cell_type":"markdown","metadata":{"id":"2V3dLr_YERyJ"},"source":["---\n","##Bias-variance tradeoff\n","\n","**Test Set** is the portion of a data set that you use to fit a given model.  These data are used to find best-fit parameters, $\\theta$, or, more generally, the posterior distribution $p(\\theta\\vert \\vec{x})$.\n","\n","**Training Set** is the portion of a data that is held-aside during the fitting procedure and is used to assess how the model generalizes to new data.\n","\n","**Bias** is the difference between the prediction of the model and the \"true\" output variables that you are trying to predict. Models with high bias will not fit the training set data well, since the predictions are quite different from the true process generating the data. These high bias models are overly simplified - they do not have enough parameters and complexity to accurately capture the patterns in the data (*underfitting*).\n","\n","\n","**Variance** refers to the variability of model predictions across different data samples. Essentially, do the model predictions change a lot with changes in the exact training data used? Models with high variance are highly dependent on the exact training data used - they will not generalize well to test data (*overfitting*).\n","\n","* High bias, low variance models have high train and test error.\n","* Low bias, high variance models have low train error, high test error\n","* Low bias, low variance models have low train and test error\n","* High bias, high variance models should just be avoided\n","\n","Having a low bias and a high variance are typically in conflict though - models with enough complexity to have low bias also tend to overfit and depend on the training data more. We need to decide on the correct tradeoff.\n","\n","In this section, we will see the bias-variance tradeoff in action with polynomial regression models of different orders.\n","\n","Graphical illustration of bias and variance.\n","(Source: http://scott.fortmann-roe.com/docs/BiasVariance.html)\n","\n","![bias-variance](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png) \n"]},{"cell_type":"markdown","metadata":{"id":"Ivw9QM6NNB2z"},"source":["##An Example: Fitting Polynomials\n","\n","A typical exersize in data analysis is to fit a polynomial to a data set.  Computing this fit is typically straight-forward (in python, you can use [```np.polyfit()```](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)), regardless of the order of the polynomial.  However, selecting how many orders is the \"best\" is often rather challenging without using a formal methodology like Bayesian model selection.\n","\n","We will start with a case where we know the answer already, data that is generated from the function $y = f(x) = 0.1x^2 + 0.5x + 2 + \\mathcal{N}(0,\\sigma^2)$, where $\\mathcal{N}(0,\\sigma^2)$ is a Gaussian random variable with mean of $0$ and standard deviation $\\sigma$."]},{"cell_type":"markdown","metadata":{"id":"3DuVb71xOyZR"},"source":["<font color=\"blue\">Question #1: Write a function, ```output_quadratic(x,sigma)```, where ```x``` is a numpy array of input values, ```sigma``` is the noise standard deviation, and the function returns ```y```, the output of applying the equation above to ```x``` (assume that the noise is independent for each value of ```x```). </font>"]},{"cell_type":"code","metadata":{"id":"L-Tawk8aPqoI"},"source":["#Write code for Question #1 here:\n","def output_quadratic(x,sigma):"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wFGPkWLVxkz8"},"source":["<font color=\"blue\">Question #2: For $\\sigma = 1$, fit and plot outputs of your function above to a linear, a quadratic, and a cubic polynomial using ```p = np.polyfit(x,y,order)```, where ```x=np.arange(-5,5,.1)``` and plot the outputs.  Compare the mean-squared-errors for all three fits (you can use [```np.polyval()```](https://numpy.org/doc/stable/reference/generated/numpy.polyval.html)).  Which fit has the lowest mean squared error?  Does this answer surprise you? Why or why not?</font>"]},{"cell_type":"code","metadata":{"id":"zn6BwOAxyu6n"},"source":["#Write code for Question #2 here:"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9_sVvPezRbt"},"source":["<font color=\"green\"> Which fit has the lowest mean squared error?  Does this answer surprise you? Why or why not? </font>"]},{"cell_type":"markdown","metadata":{"id":"SRIef1gl1Gln"},"source":["<font color=\"blue\"> Question #3: Now, regenerate ```y``` 10,000 times using your function from Question #1 (still, $\\sigma = 1$) and fit a quadratic polynomial to a training set that consists of a random 90% of the values (90 points out of 100).  Save the fitted parameter values, the training set mean-squared-error (MSE), and the test set MSE each time.  Plot histograms for the two MSEs.  Is your training or test MSE higher?  Why is this?  Do the parameter estimates appear biased to you? (Remember: the model generating the data is $y = f(x) = 0.1x^2 + 0.5x + 2 + \\mathcal{N}(0,\\sigma^2)$)"]},{"cell_type":"code","metadata":{"id":"H-Up4qzm5t5j"},"source":["#Type your code for Question #3 here\n","#Hint: you can use:\n","#   idx = np.arange(np.shape(x)[0])\n","#   random.shuffle(idx); \n","#   x_train = x[idx[:L]];y_train = y[idx[:L]];x_test = x[idx[L:]];_test = y[idx[L:]]\n","#to generate a random test and training set (where L = 90, here)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pl_bPylf5z5T"},"source":["<font color=\"green\"> Is your training or test MSE higher?  Why is this?  Do the parameter estimates appear biased to you?</font>"]},{"cell_type":"markdown","metadata":{"id":"5cx5k6Uy8pvV"},"source":["<font color=\"blue\"> Question #4: Repeat Question #3, but with cubic fits instead of quadratic fits.  Compare the means and variances of the training and test set MSEs with those that you got from the quadratic fits.  Based on these results, which fit would you pick (pretend that you don't know the answer)?  Justify your answer."]},{"cell_type":"code","metadata":{"id":"fmazsoVv8zzE"},"source":["#Type your code for Question #4 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DttoOskxFSXS"},"source":["<font color=\"green\"> Compare the means and variances of the training and test set MSEs with those that you got from the quadratic fits. Based on this information, which fit would you pick (pretend that you don't know the answer)? Justify your answer.</font>"]},{"cell_type":"markdown","metadata":{"id":"NhsQhY1yJnkW"},"source":["<font color=\"blue\">Question #5: Plot the mean and standard deviation for the test and training MSEs for polynomial fits from order 1 to 10 ($\\sigma = 1$, still).  For the sake of time, you can reduce the number of instantiations per polynomial to 1,000 instead of 10,000.  Based on these results, which order would you pick?  Why?  Does this agree with the correct result?  Discuss your results in light of the bias-variance trade-off. (You can use [```plt.errorbar()```](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.errorbar.html) to make your plots).</font>"]},{"cell_type":"code","metadata":{"id":"VkHWShSc8pGu"},"source":["#Type code for Question #5 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImmBHyZbNxPd"},"source":["<font color=\"green\"> Based on these results, which order would you pick? Why? Does this agree with the correct result? Discuss your results in light of the bias-variance trade-off.</font>"]},{"cell_type":"markdown","metadata":{"id":"JdJOG4pHOTXQ"},"source":["<font color=\"blue\"> Question #6: Repeat the previous question but for $\\sigma = 5$.  How do you results change?  Why do you think these effects are occurring?"]},{"cell_type":"code","metadata":{"id":"1vC-k5SHOLXA"},"source":["#Type code for Question #6 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNTFNJWYO48L"},"source":["<font color=\"green\"> How do you results change?  Why do you think these effects are occurring? </font>"]},{"cell_type":"markdown","metadata":{"id":"0kupjUjaNX66"},"source":["##Estimating errors and maximum likelihood"]},{"cell_type":"markdown","metadata":{"id":"PJ3iVYvCNXzS"},"source":["Mean-squared-error, while a useful measure, doesn't fit directly into a Bayesian framework, which is based purely on probabilities.  For this, we need to translate MSE into a *likelihood* -  $p(\\text{Data}\\vert\\text{Model})$, which we will call $p(X\\vert \\mathcal{M},\\theta)$.  This is done by assuming that the data we see is generated from a true underlying model and is then corrupted by a noise distribution that we know.  We can also make our lives easier by assuming that each data point is arrived at independently. Thus, our likelihood is aproximated by:\n","\\begin{equation}\n","p(X\\vert \\mathcal{M},\\theta) \\approx p(x_1\\vert \\mathcal{M},\\theta)p(x_2\\vert \\mathcal{M},\\theta)\\ldots p(x_N\\vert \\mathcal{M},\\theta) = \\prod_{i=1}^N p(x_i\\vert \\mathcal{M},\\theta),\n","\\end{equation}\n","where each $x_i\\in X$ is a seperate data point.  Or, if we write this in terms of the log-likelihood, $\\mathcal{L}(X\\vert \\mathcal{M},\\theta)$,\n","\\begin{equation}\n","\\mathcal{L}(X\\vert \\mathcal{M},\\theta) \\approx \\log p(x_1\\vert \\mathcal{M},\\theta)+\\log p(x_2\\vert \\mathcal{M},\\theta) + \\ldots + \\log p(x_N\\vert \\mathcal{M},\\theta) = \\sum_{i=1}^N \\log p(x_i\\vert \\mathcal{M},\\theta),\n","\\end{equation}\n","\n","Although the distribution $p(x_i\\vert \\mathcal{M},\\theta)$ could have many potential forms, due ot the central limit theorem, the most common form for this is a gaussian distribution centered around our model's estimate, $\\hat{f}(x_i\\vert \\mathcal{M},\\theta)$:\n","\\begin{equation}\n","p(x_i\\vert \\mathcal{M},\\theta) = \\frac{1}{\\sqrt{2\\pi \\hat{\\sigma}^2}}\\exp \\left [\\frac{-(x_i-\\hat{f}(x_i\\vert \\mathcal{M},\\theta)^2}{2\\hat{\\sigma}^2} \\right],\n","\\end{equation}\n","where $\\hat{\\sigma}^2$ is the measured mean-squared error of the fit.\n","\n","It should be noted that the mean-squared error often isn't a good assumption in the case of high-bias/low-variance models that underfit the data (e.g., the linear fit back in Question #1), but it is a good assumption when comparing models that don't obviously under-fit the data.\n","\n","Nonetheless, if we plug this definition into the previous equation (and play around with logrithms and exponentials), we find that:\n","\\begin{equation}\n","\\mathcal{L}(X\\vert \\mathcal{M},\\theta) = \\sum_{i=1}^N \\log p(x_i\\vert \\mathcal{M},\\theta) = -\\frac{N}{2}\\log(2\\pi\\hat{\\sigma}^2) - \\frac{N}{2}.\n","\\end{equation}\n","Thus, as our MSE ($\\hat{\\sigma}^2$) decreases, $\\mathcal{L}(X\\vert \\mathcal{M},\\theta)$ increases, as the likelihood becomes less negative.  Thus, minimizing MSE and maximizing likelihood are equivalent!"]},{"cell_type":"markdown","metadata":{"id":"kbOoQv6gZAvm"},"source":["<font color=blue>Question #7: Write a function, ```calculateLikelihood(x,y,order)```, that fits a polynomial of order ```order``` to numpy arrays ```x``` and ```y``` and returns the likelihood of generating the observed data given the fitted model (using the equation above).</font>"]},{"cell_type":"code","metadata":{"id":"lSmA5a-qbfdU"},"source":["#Write code for Question #7 here\n","def calculateLikelihood(x,y,order):"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrB3ISiZdSjT"},"source":["<font color=blue>Question #8: Use your functions from Questions #1 and #7 to calculate the likelihoods for polynomial fits from order 1 to 10 for a single instantiation of the data.  Use ```x=np.arange(-5,5,.1)``` and $\\sigma=1$.  Based soley on the likelihood, which model would you pick?  Why?</font>"]},{"cell_type":"code","metadata":{"id":"qbbeknyaeBRB"},"source":["#Write code for Question #8 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYrnwmkYkWzy"},"source":["<font color=green>Based soley on the likelihood, which model would you pick?  Why? </font>"]},{"cell_type":"markdown","metadata":{"id":"m8W8J-mVjmuJ"},"source":["##The Akaike Information Criteron (AIC)"]},{"cell_type":"markdown","metadata":{"id":"mgiFIFYykEc0"},"source":["As discussed in class and in the previous sections of this notebook, however, just minimizing the likelihood often leads to low-bias/high-variance models (e.g., overfitting).  Really, we want to minimize the error on (or, more generally, maximize the likelihood of generating) new data that the model has not seen during fitting.  The Akaike Information Critereon (AIC) is a way of estimating this likehood in the case of $N\\to\\infty$.  Specifically, if the model is trained on data $X$ and then sees new data $X^{(0)}$ generated by the same process as $X$,\n","\\begin{equation}\n","\\mathcal{L}(X^{(0)}\\vert \\mathcal{M},\\theta)\\approx \\frac{1}{N}\\mathcal{L}(X\\vert \\mathcal{M},\\hat\\theta) - \\frac{p}{N},\n","\\end{equation}\n","where $\\mathcal{M}$ is the model in question, $\\hat{\\theta}$ is the parameter set that optimizes the likelihood of $X$, $N$ is the number of data points, and $p$ is the number of parameters in the model. \n","\n","Given this observation, then we usually define the AIC to be -2N times this quantity:\n","\\begin{equation}\n","\\text{AIC} = -2\\mathcal{L}(X\\vert \\mathcal{M},\\hat\\theta) +  2p.\n","\\end{equation}\n","Thus, a smaller value of the AIC implies a larger likelihood on the new data, and, thus, a  model that generalizes better.  It should be noted, however, that corrections need to be made in cases where $N$ is not effectively infinite, and there are many methods that have been developed to adjust for these effects in different ways."]},{"cell_type":"markdown","metadata":{"id":"7AgDovOupkCm"},"source":["<font color=blue>Question #9: Use your functions from Questions  #1 and #7 to calculate and plot the AIC for polynomial fits of order 1 through 10 for a single instantiation of the data (same $\\sigma$ and ```x``` as the previous question).  Repeat the analysis for random subsamples of 60% and 90% the data. How do your results change?  What might be causing this? </font>"]},{"cell_type":"code","metadata":{"id":"1XOHV6BJne9a"},"source":["#Type your code for Question #9 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W8AQ7pU7qZkQ"},"source":["<font color=green> How do your results change?  What might be causing this? </font>"]},{"cell_type":"markdown","metadata":{"id":"fw4Q5RTpq8jo"},"source":["##The Bayesian Information Criteron (BIC)\n","\n","While the AIC provides an improved estimate of a model's generalizability, it remains difficult to translate that estimate into a *posterior distribution* - $p(\\mathcal{M}\\vert X)$ - of a model given the data.  Written more explicitly,\n","\\begin{equation}\n","p(\\mathcal{M}\\vert X) \\propto p(\\mathcal{M})p(X\\vert\\mathcal{M})=p(\\mathcal{M})\\int p(X\\vert\\mathcal{M},\\theta)p(\\theta\\vert\\mathcal{M})d\\theta.\n","\\end{equation}\n","If we assume that $p(X\\vert\\mathcal{M},\\theta)$ is sufficiently \"peaky\" near its maximum, $\\hat\\theta$, then we can use Laplace's Method to approximate the integral, so that the log-likehood, $\\mathcal{L}(X\\vert\\mathcal{M})$ is approximated by\n","\\begin{equation}\n","\\mathcal{L}(X\\vert\\mathcal{M}) \\approx \\mathcal{L}(X\\vert\\mathcal{M},\\hat\\theta) - \\frac{p}{2}\\log N,\n","\\end{equation}\n","where $p$, again, is the number of parameters, and $N$ is the number of data points.\n","\n","Mutiplying by -2, we thus define the Bayesian Information Critereon (BIC) as:\n","\\begin{equation}\n","\\text{BIC} = -2\\mathcal{L}(X\\vert\\mathcal{M},\\hat\\theta) + p\\log N.\n","\\end{equation}\n","Thus, the smaller the BIC, the higher the posterior distrtibution of the model, and the more likely that the model is the \"true\" model of the data.  \n","\n","Moreover, because the the BIC is proportional to -2 times the log-posterior, if given a set of models, $\\mathcal{M}_1,\\mathcal{M}_2,\\ldots,\\mathcal{M}_k$, then if we assume a uniform prior over models, we can calculate the posterior distribution over our models via:\n","\\begin{equation}\n","p(\\mathcal{M}_i\\vert X) = \\frac{exp\\left[-\\frac{1}{2} \\text{BIC}(\\mathcal{M}_i)\\right ]}{\\sum_{\\ell=1}^k exp\\left[-\\frac{1}{2} \\text{BIC}(\\mathcal{M}_\\ell)\\right ]}.\n","\\end{equation}\n","\n","Note that, often, we are actually uncertain about the parameters that maximize $p(X|\\mathcal{M},\\theta)$ (this was the whole point of Bayesian inference!), so in many cases, the assumptions that underlie the BIC do not hold, and we need to estimate the full integral in the first equation of this section using Monte Carlo or other methods."]},{"cell_type":"markdown","metadata":{"id":"Uy7V_OT8vsFO"},"source":["<font color=blue>Question #10: Use your functions from Questions  #1 and #7 to calculate and plot the BIC for polynomial fits of order 1 through 10 for a single instantiation of the data (same $\\sigma$ and ```x``` as the previous question).  Repeat the analysis for random subsamples of 60% and 90% the data. How do your results compare to the AIC results? What might be causing these differences?</font>"]},{"cell_type":"code","metadata":{"id":"66H4xeVLv5sR"},"source":["#Type your code for Question #10 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAHvOEuJxH44"},"source":["<font color=green> How do your results compare to the AIC results? What might be causing these differences? </font>"]},{"cell_type":"markdown","metadata":{"id":"x4R9T1ZGxPbE"},"source":["<font color=blue>Question #11: Use your results question #10 to compute and plot the posterior distributions over the 10 models for all three cases.  Hint: to keep your code from getting infinities, you can subtract the maximum value of the BIC from each of the values before computing the posterior.  Are your results more or less certain as you increase the number of data points?</font>"]},{"cell_type":"code","metadata":{"id":"yC3eEwsNxdUB"},"source":["#Type your code for Question #11 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KqUK2gvZzumN"},"source":["<font color=green> Are your results more or less certain as you increase the number of data points? </font>"]},{"cell_type":"markdown","metadata":{"id":"aLflQ8qX18hR"},"source":["##Fitting unknown data"]},{"cell_type":"markdown","metadata":{"id":"yutfp-q3KwFO"},"source":["For this portion of the exercise, you will now be given data from an unknown polynomial that has been corrupted with an unknown amount of noise.  Run the code below to import the data.  ```x``` are the x-axis values, and ```y1```, ```y2```, and ```y3``` are three different instantiations of the data set (same polynomial, same amount of noise)."]},{"cell_type":"code","metadata":{"id":"-cokBZ-DKci1"},"source":["url = 'https://raw.githubusercontent.com/gordonberman/bioqtm385_fall2020/master/data/test_data.csv'\n","test_data_df = pd.read_csv(url,header=None)\n","test_data = test_data_df.to_numpy()\n","x = test_data[:,0]\n","y1 = test_data[:,1]\n","y2 = test_data[:,2]\n","y3 = test_data[:,3]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mcD2jBYLLWYg"},"source":["<font color=\"blue\">Question #12: Using the data above, what can you say about the underlying polynomial that generated the data?  Use model-selection-based arguments to back-up your assertions.</font>"]},{"cell_type":"code","metadata":{"id":"nxSJFv67MGD7"},"source":["#Type your code for Question #12 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BUVxGsNTMOFV"},"source":["<font color=green> What can you say about the underlying polynomial that generated the data? Use model-selection-based arguments to back-up your assertions. </font>"]},{"cell_type":"markdown","metadata":{"id":"mrUjYW5Nz5hW"},"source":["##Spike sorting revisited"]},{"cell_type":"markdown","metadata":{"id":"83Qcv8d-0C6K"},"source":["Use this code to load the spiking data from in-class exercise 3b."]},{"cell_type":"code","metadata":{"id":"wVWIq2E30JMC"},"source":["url = 'https://raw.githubusercontent.com/gordonberman/bioqtm385_fall2020/master/data/spike_data.csv'\n","spike_data_df = pd.read_csv(url,header=None)\n","spike_data = spike_data_df.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18i-ScQe0VTv"},"source":["<font color=blue> Question #13: Revisit Question #2 from in-class exercise 3b.  Knowing what you know now, use ideas from model selection to estimate the number of neurons in the data set.  Justify your answer.  Note that ```aic``` and ```bic``` can be [calculated](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) directly from GaussianMixture fits."]},{"cell_type":"code","metadata":{"id":"EfIv-ukj1lY9"},"source":["#Type code for Question #13 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JMDxMPC12sm"},"source":["<font color=\"green\"> Knowing what you know now, use ideas from model selection to estimate the number of neurons in the data set. Justify your answer.  </font>"]}]}